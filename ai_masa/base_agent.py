import sys
import json
import subprocess
from .models.message import Message
from .comms.redis_broker import RedisBroker
from .prompts import PROMPT_TEMPLATES

class BaseAgent:
    def __init__(self, name, role_prompt, redis_host='localhost', language='ja', llm_command=None):
        self.name = name
        self.role_prompt = role_prompt
        self.context = []
        self.language = language
        self.llm_command = llm_command
        
        # RedisBrokerã‚’ä½¿ç”¨
        self.broker = RedisBroker(host=redis_host)
        self.broker.connect()

    def observe_loop(self):
        """å—ä¿¡å¾…æ©Ÿãƒ«ãƒ¼ãƒ—é–‹å§‹"""
        print(f"[{self.name}] Listening on Redis...")
        self.broker.subscribe(self._on_message_received)

    def _on_message_received(self, message_json):
        """å—ä¿¡æ™‚ã®ã‚³ãƒ¼ãƒ«ãƒãƒƒã‚¯"""
        try:
            msg = Message.from_json(message_json)
            
            if msg.from_agent == self.name:
                return

            is_to_me = msg.to_agent == self.name
            is_cc_me = self.name in msg.cc_agents

            if is_to_me or is_cc_me:
                self.context.append(msg)
                
                if is_to_me:
                    print(f"[{self.name}] ğŸ“¨ Received from {msg.from_agent}: {msg.content}")
                    self.think_and_respond(msg)
                else:
                    print(f"[{self.name}] ğŸ‘€ (CC) Saw message from {msg.from_agent}")

        except Exception as e:
            print(f"[{self.name}] Error: {e}")

    def think_and_respond(self, trigger_msg):
        """LLMã®æ€è€ƒã‚’ãƒˆãƒªã‚¬ãƒ¼ã—ã€å¿œç­”ã‚’ç”Ÿæˆãƒ»é€ä¿¡ã™ã‚‹"""
        prompt = self._build_prompt(trigger_msg)
        llm_response_json = self._invoke_llm(prompt)
        
        if not llm_response_json:
            print(f"[{self.name}] Error: LLM did not return a response.")
            return

        try:
            response_data = json.loads(llm_response_json)
            self.broadcast(
                target=response_data.get("to_agent"),
                content=response_data.get("content"),
                cc=response_data.get("cc_agents")
            )
        except json.JSONDecodeError as e:
            print(f"[{self.name}] Error decoding LLM response: {e}")
            print(f"[{self.name}] Received: {llm_response_json}")
        except Exception as e:
            print(f"[{self.name}] Error processing LLM response: {e}")

    def _build_prompt(self, trigger_msg):
        """LLMã¸ã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’æ§‹ç¯‰ã™ã‚‹"""
        history = "\n".join([f"- {msg.from_agent}: {msg.content}" for msg in self.context])
        
        template = PROMPT_TEMPLATES.get(self.language, PROMPT_TEMPLATES['en'])
        
        return template.format(
            name=self.name,
            role_prompt=self.role_prompt,
            history=history,
            from_agent=trigger_msg.from_agent,
            content=trigger_msg.content
        )

    def _invoke_llm(self, prompt):
        """LLMã‚’å‘¼ã³å‡ºã™ã€‚llm_commandãŒæŒ‡å®šã•ã‚Œã¦ã„ã‚Œã°å¤–éƒ¨ã‚³ãƒãƒ³ãƒ‰ã¨ã—ã¦ã€ãªã‘ã‚Œã°ãƒ€ãƒŸãƒ¼ã‚’è¿”ã™"""
        print(f"[{self.name}] ğŸ§  Thinking...")

        if self.llm_command:
            try:
                # å¤–éƒ¨ã‚³ãƒãƒ³ãƒ‰ã‚’å®Ÿè¡Œã—ã€æ¨™æº–å…¥åŠ›ã«ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’æ¸¡ã—ã€æ¨™æº–å‡ºåŠ›ã‚’å—ã‘å–ã‚‹
                process = subprocess.run(
                    self.llm_command,
                    input=prompt,
                    capture_output=True,
                    text=True,
                    shell=True,
                    check=True
                )
                # ãƒ­ã‚°å‡ºåŠ›ã®ãŸã‚ã«ã€ä¸€åº¦JSONã‚’ãƒ‡ã‚³ãƒ¼ãƒ‰ãƒ»å†ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰ã—ã¦è¦‹ã‚„ã™ãã™ã‚‹
                try:
                    pretty_stdout = json.dumps(json.loads(process.stdout), ensure_ascii=False)
                    print(f"[{self.name}] LLM command stdout: {pretty_stdout[:300]}")
                except json.JSONDecodeError:
                    # JSONã¨ã—ã¦ãƒ‘ãƒ¼ã‚¹ã§ããªã„å ´åˆã¯ãã®ã¾ã¾å‡ºåŠ›
                    print(f"[{self.name}] LLM command stdout: {process.stdout[:300]}")
                return process.stdout
            except subprocess.CalledProcessError as e:
                print(f"[{self.name}] Error executing LLM command: {e}")
                print(f"[{self.name}] Stderr: {e.stderr}")
                return None
            except FileNotFoundError:
                print(f"[{self.name}] Error: LLM command not found: '{self.llm_command}'")
                return None
        else:
            # llm_commandãŒæŒ‡å®šã•ã‚Œã¦ã„ãªã„å ´åˆã®ãƒ€ãƒŸãƒ¼ãƒ¬ã‚¹ãƒãƒ³ã‚¹
            print(f"[{self.name}] (Using dummy response)")
            dummy_response = {
                "to_agent": "dummy_agent",
                "cc_agents": [],
                "content": "This is a dummy response as llm_command is not set."
            }
            return json.dumps(dummy_response, ensure_ascii=False)

    def broadcast(self, target, content, cc=None):
        if not target or not content:
            print(f"[{self.name}] âš ï¸ Missing target or content. Aborting broadcast.")
            return
        msg = Message(self.name, target, content, cc_agents=cc)
        self.broker.publish(msg.to_json())
        print(f"[{self.name}] ğŸš€ Sent to {target}: {content}")

if __name__ == "__main__":
    if len(sys.argv) < 3:
        print("Usage: python -m ai_masa.base_agent <Name> <Role> [language] [llm_command]")
        sys.exit(1)
    
    name = sys.argv[1]
    role = sys.argv[2]
    lang = sys.argv[3] if len(sys.argv) > 3 else 'ja'
    cmd = sys.argv[4] if len(sys.argv) > 4 else None

    agent = BaseAgent(name, role, language=lang, llm_command=cmd)
    agent.observe_loop()
